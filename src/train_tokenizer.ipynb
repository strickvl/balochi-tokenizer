{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# load_dataset(\"balochiml/balochi-language-data\", data_dir=\"data\", cache_dir=\"../data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the processed data without English characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_txt_file_paths(directory):\n",
    "    txt_file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                txt_file_paths.append(file_path)\n",
    "    return txt_file_paths\n",
    "\n",
    "\n",
    "# Replace \"directory_path\" with the actual path of the directory you want to search\n",
    "directory_path = \"../data/raw_text\"\n",
    "txt_paths = get_txt_file_paths(directory_path)\n",
    "\n",
    "len(txt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(file_path):\n",
    "    # Open the file and read it into memory\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Remove English-language characters and numbers\n",
    "    text = re.sub(r\"[a-zA-Z0-9]\", \"\", text)\n",
    "\n",
    "    # Remove any excess whitespace\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in txt_paths:\n",
    "    cleaned_text = clean_text(path)\n",
    "\n",
    "    # write the cleaned text to a new file with an incremented filename\n",
    "    # write the files all into the '../data/processed_text' directory\n",
    "    with open(\n",
    "        f'../data/processed_text/{path.split(\"/\")[-1]}', \"w\", encoding=\"utf-8\"\n",
    "    ) as file:\n",
    "        file.write(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer using ğŸ¤— Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    min_frequency=2,\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all the txt files in\n",
    "# '/Users/strickvl/balochi/balochi-tokenizer/data/processed_text'\n",
    "\n",
    "processed_files = get_txt_file_paths(\"../data/processed_text\")\n",
    "assert len(processed_files) == len(txt_paths)\n",
    "len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(processed_files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x108eaa830>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ú¯ÙˆÚº Ú¾Ø± Ú©Ø³ Ø¡Ù Ø¬Ù†Ú¯ Ø¡ Ù Ù…Ú‘ Ø¨ÛŒØª'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Â  Â  Â  Ø¢ÛŒÚ©Â  Ø¬Ù†Ø§ÙˆØ±Û’ Ø§ÙØªÛ”Â  Ù„Ú¾ØªÛ’ Ú¯Ø´ÛŒØª Ø¢ Ø³Ú©ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„Û’ Ø§Øª Ú©Û Ø§Ú¯Ø§Úº Ø¢Ø²Ø§ØªÛŒ Ø¯ÛŒÚ¯ Ø¨Û Ø¨ÛŒØªØŒ Ø¨Ø§Ø²Ø§Ø±Ø¡ÙØŒ Ù„ÙˆÚ¯Û’ Ø¡ÙØŒ Ø¬Ø§Ú¯Ø§Û ÛŒÛ’Â  Ø¡ÙØŒØ¯Ù¾ØªØ± Ø¡ Ù Ú©Ø§Ø±Ú¯Ø³ ÛŒÛ’Â  Ø¡Ù ÛŒØ§ Ú¾Ø± Ú¾Ù…Ø§ Ø¬Ø§Ú¯Ø§Û Ø¡Ù Ú©Û Ø´ÙØª Ú©Ù†Øª Ù…Ø²Ù†ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú©Ù†ØªÛ”Ú¯ÙˆÚº Ú¾Ø± Ú©Ø³ Ø¡Ù Ø¬Ù†Ú¯ Ø¡ Ù Ù…Ú‘ Ø¨ÛŒØªÛ”Ú¯Ø¯Ø¡ Ù Ù¾Ú†Ø§ÚºÂ  Ú†Ù†Úˆ Ú†Ù†Úˆ Ø¡ Ù Ø±Ø§Ú‘ Ø±Ø§Ú‘ Ú©Ù†ØªØŒÚ©Ø§Ú¯Ø¯ Ø¡ Ù ÙˆØ§Ù†Ú¯ÛŒØ§Úº ÙˆØ§Ø±Øª Ø¡ Ù Ø¢Ø¯Ø±Ø§Û Ú©Ù†ØªÛ”ÙˆØ±Ú¯ÛŒ Ú†ÛŒØ²Ø§Úº Ø§Ú¯Ø§Úº ÙˆØ§Ø±Øª Ù†Ú©Ù†Øª Ø¢Ú¾Ø§Úº Ú¯Ù¹ Ù¾Ø§Ú†ÛŒØª Ú¾Ø±Ø§Ø¨ Ú©Ù†ØªÛ”Ø§ÛŒÙ†Ø¯Ú¯Û Ø¬Ù†Ø§ÙˆØ± Ú†Û Ø¨Ù†Ø¯Ø§Øª Ø¡ Ù Ø§ÛŒØ´ÛŒ Ø¡Ù Ú©Ø§Ø²ÙˆØ§Ù„ÛŒØ§Úº Ú†Û ÙˆØªØ§ Ø¯ÛŒØ± Ø¯Ø§Ø±Ú¯ Ø¡Ù Ú©ÙˆØ´Ø³Øª Ú©Ù† Ø§ÙÙ†ØªÛ” Ú†ÛŒØ§ Ú©Û Ø¢ Ø¨Ø§Ø²ÛŒÚº Ø¯Ú¯Û Ú¾Ø±Ø§Ø¨ÛŒ Ø¡ Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú¾Ù… Ú©Ù†ØªØŒÙ¾Ù…ÛŒØ´Ú©Ø§ Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±Â  Ø¨Ø§Ù„ÛŒ Ù…ÙØ±Ú¯ØŒÚ©ÙˆÛ Ù¾Ø§Ú†Ù†ØŒØ¢Ø³Ú© Ø¡ Ù Ø§ÛŒÙ†Ø¯Ú¯Û Ú©Ø³Ø§Ù† Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±Ú†Ø± Ø¢Ø¦ÛŒ Ø¡Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒØ§Ù†ÛŒ Ø³ÙˆØ¨ Ø¡Ù Ø¢Ø¦ÛŒ Ø¡Ù Ú†Û Ø³Ú© Ø¨Ø§Ø² Ø´Ø²Ø§Ø± Ø§ÙÙ†Øª Û”\".replace(\n",
    "    \"\\xa0\", \"\"\n",
    ")\n",
    "sample_sentence = sample_text.split(\"Û”\")[2]\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ú¯ÙˆÚº', 'Ú¾Ø±', 'Ú©Ø³', 'Ø¡Ù', 'Ø¬Ù†Ú¯', 'Ø¡', 'Ù', 'Ù…Ú‘', 'Ø¨ÛŒØª']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sample_sentence).tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a custom tokenizer using Spacy and FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "files = get_text_files(\"../data/processed_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*Ø¢Ù…ÛŒØªÚ¯Ø¡Ù Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†* Ù„Ú†Ù‘Û: *Ø¢Ù…ÛŒØªÚ¯Ø¡Ù Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†* Ø¢ Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û Ù…Ù† ÙˆØªÛŒ Ø´ÙˆÚ©ÛŒÚº Ú©Ø³Ø§Ù†ÛŒ'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#146) ['*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ø¢','Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û','Ù…Ù†','ÙˆØªÛŒ','Ø´ÙˆÚ©ÛŒÚº','Ú©Ø³Ø§Ù†ÛŒ','Ù¾ÛŒØ±','Ú©ÙØª','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬Ø³ÙØªØ¡ÙÙ…Ú©Ù†','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ú¯ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¨Û’','Ø§ÙˆØ³ØªÛŒÚº','ØªØ§Ù‡ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¢'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#147) ['xxbos','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ø¢','Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û','Ù…Ù†','ÙˆØªÛŒ','Ø´ÙˆÚ©ÛŒÚº','Ú©Ø³Ø§Ù†ÛŒ','Ù¾ÛŒØ±','Ú©ÙØª','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬Ø³ÙØªØ¡ÙÙ…Ú©Ù†','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ú¯ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¨Û’','Ø§ÙˆØ³ØªÛŒÚº','ØªØ§Ù‡ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¢'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(size: int):\n",
    "    sp = SubwordTokenizer(vocab_sz=size)\n",
    "    sp.setup(txts)\n",
    "    return \" \".join(first(sp([txt]))[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â–* Ø¢ Ù…ÛŒ ØªÚ¯ Ø¡Ù â–Ø¬ÙØ³Øª Ø¡Ù Ù… Ú© Ù† * â–Ù„Ú†Ù‘Û : â–* Ø¢ Ù…ÛŒ ØªÚ¯ Ø¡Ù â–Ø¬ÙØ³Øª Ø¡Ù Ù… Ú© Ù† * â–Ø¢ â–Ù…ÛŒØªÚ¯ Ø¡Ù Ú©Û â–Ù…Ù† â–ÙˆØªÛŒ â–Ø´ ÙˆÚ©ÛŒÚº â–Ú©Ø³ Ø§Ù†ÛŒ â–Ù¾ÛŒØ± â–Ú©ÙØª â–Ø¢ â–Ù…ÛŒØªÚ¯ Ø¡Ù â–Ø¬'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â– * Ø¢ Ù… ÛŒ Øª Ú¯ Ø¡ Ù â– Ø¬ Ù Ø³ Øª Ø¡ Ù Ù… Ú© Ù† * â– Ù„ Ú† Ù‘ Û : â– * Ø¢ Ù… ÛŒ Øª Ú¯ Ø¡ Ù â– Ø¬ Ù Ø³ Øª'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#147) ['xxbos','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†'...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#4096) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','Ø¡Ù','Ø¡Ù','Ø¡Ù','Û”','Ú©Û','ØŒ','Ø§Ù†Øª','Ù…Ù†','Ø§Û’','Ù†Û','ÙˆØªÛŒ','Ø¨ÛŒØª','â€','Ø§Øª','Ú†Û','Ú¯ÙˆÚº','Ø§ÙÙ†Øª','Ø§ÙÙ†Øª','Ù¾Û','Ø¨Û','â€˜','ÛŒÚ©','Ø¢Ø¦ÛŒ','.','Ø¢','Ù…Ù†ÛŒ','Ú¾Ù…',')','Ú©Ù†Øª','Ø¨Ù„ÙˆÚ†ÛŒ','3','ØªÙˆ','Ø¨Ù„Û’','Ø¦Û’',':','Ú©Ù†Ú¯','(','Ø¨ÙˆØªÚ¯','Ø¢Úº','Ú©Ù†','ØŸ'...]\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([ 156, 2340,    0,  156,  563,   43,  156, 2340,    0,  156,   33,\n",
       "               0,   16,   19, 1490,  831,  457,  102,   33, 1031])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* Ø¢Ù…ÛŒØªÚ¯Ø¡Ù xxunk * Ù„Ú†Ù‘Û : * Ø¢Ù…ÛŒØªÚ¯Ø¡Ù xxunk * Ø¢ xxunk Ù…Ù† ÙˆØªÛŒ Ø´ÙˆÚ©ÛŒÚº Ú©Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ± Ú©ÙØª Ø¢ Ù…ÛŒØªÚ¯Ø¡Ù'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "balochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
